# Practical Machine Learning Prediction Assignment

## Synopsis  
  
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks.  

One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.  

The goal of this report is to predict the manner in which the participants did the exercise, this is the "classe" variable in the training set. This report describes how the model for the project was built, its cross validation, expected out of sample error calculation, and the choices made.  

## Data Processing  
First we set the seed for our report, then download and read in the test and training data sets.  

```{r cache=TRUE}
## Set the seed to allow for reproducibility
set.seed(11132009)

if(!file.exists("training-set.csv")){
    trainURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
    download.file(trainURL, "training-set.csv", method="curl")
}

if(!file.exists("testing-set.csv")){
    testURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
    download.file(testURL, "testing-set.csv", method="curl")
}

training <- read.csv("training-set.csv", na.strings = c("NA", "#DIV/0!", ""))
testing  <- read.csv("testing-set.csv",  na.strings = c("NA", "#DIV/0!", ""))
```
  
We do some basic evaluations of the training set to get an understanding of the data. In particular we look at the "classe" column.  

```{r}
str(training, list.len=15)
table(training$classe)
prop.table(table(training$classe))
```

Based on the above information we see that the first 6 columns are general information and can be removed for both the testing and training data sets. We also go ahead and remove the columns that are mostly NA's at this time.  

```{r}
training <- training[, 7:160]
testing  <- testing[, 7:160]
dim(training)
dim(testing)

no.na  <- apply(!is.na(training), 2, sum) > 19621
training <- training[, no.na]
testing  <- testing[, no.na]
dim(training)
dim(testing)
```
At this point we should also eliminate predictors with a near zero variance, but a call to nearZeroVar on the training set returns zero rows, which means that all our current predictors have a significant variance and do not need to be removed.  

We now go ahead and separate our training data set into two for cross validation purposes.  

```{r}
library(caret)

inTrain <- createDataPartition(y=training$classe, p=0.60, list=FALSE)
train1  <- training[inTrain,]
train2  <- training[-inTrain,]
dim(train1)
dim(train2)
```

## Results  

We check for correlation between the predictors using heatmap for a visualization.  
```{r}
train1Cor <- cor(train1[, names(train1) != "classe"])
heatmap(train1Cor)
```
  
Based on the above coloring we can see that there are some correlations, but overrall the correlations are not high and will be within parameters for our predictive model.  

We attempt to narrow down our covariates even further using PCA to reduce the number of predictors.   

```{r}
PCAmodel <- preProcess(train1[,-54], method="pca", thresh=0.90)
train1PCA <- predict(PCAmodel, train1[,-54])
train2PCA <- predict(PCAmodel, train2[,-54])

train1PCA$classe <- train1$classe
train2PCA$classe <- train2$classe
```

This reduces our predictors to 19 which will make the training time much shorter.  
  
We can now build our model based off our train1PCA data set. We will be using Random Forest for our method as it is tolerant to non-linearity and some correlation.  
```{r cache=TRUE}
tc <- trainControl(method = "cv", number = 4, allowParallel = TRUE)

model <- train(classe ~ ., data = train1PCA, method = "rf", trControl = tc)
```

We then check the model and calculate the prediction accuracy.
```{r}
model

trainingAccuracy <- round(max(model$results$Accuracy), 4) * 100
trainingAccuracy
```

Lastly, we cross-validate training with the train2 sub-sampling we split from the data set.
```{r}
prediction.training <- predict(model, train2PCA)

matrix <- confusionMatrix(prediction.training, train2PCA$classe)
matrix
```

Based on the matrix is appears the overall accuracy we achieved was 97.2%. This would mean that our out of sample error was 2.8%.  


